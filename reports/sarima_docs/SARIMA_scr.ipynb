{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np  # vectors and matrices\n",
        "import pandas as pd  # tables and data manipulations\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from prepare_data import get_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def anomalies_bonds(series, window=4, scale=1.96):\n",
        "    \n",
        "    result = pd.DataFrame()\n",
        "    rolling_mean = series.rolling(window=window).mean()\n",
        "    result['predict_volume'] = rolling_mean\n",
        "    mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
        "    deviation = np.std(series[window:] - rolling_mean[window:])\n",
        "    lower_bond = rolling_mean - (mae + scale * deviation)\n",
        "    upper_bond = rolling_mean + (mae + scale * deviation)\n",
        "    result['lower_bond'] = lower_bond\n",
        "    result['upper_bond'] = upper_bond\n",
        "    anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n",
        "    anomalies[series < lower_bond] = series[series < lower_bond]\n",
        "    anomalies[series > upper_bond] = series[series > upper_bond]\n",
        "    result['anomalies'] = anomalies\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def db1_sarima_anomalies_detect(dataframe, table):\n",
        "    print(f'Prepare catch_{table}_sarima_predict.csv')\n",
        "\n",
        "    ids = pd.Series(dataframe[f'id_{table}']).unique()\n",
        "    rate = pd.DataFrame()\n",
        "    for id in ids:\n",
        "        aggr_df = dataframe.loc[dataframe[f'id_{table}']==id].groupby(\"date\")[[\"catch_volume\"]].sum()\n",
        "        if aggr_df.size > 20:\n",
        "            anomalies = anomalies_bonds(aggr_df)    \n",
        "            aggr_df[f'id_{table}'] = id\n",
        "            aggr_df = pd.concat([aggr_df, anomalies], axis=1)\n",
        "            aggr_df = aggr_df.reset_index()\n",
        "            rate = pd.concat([rate, aggr_df], axis=0)\n",
        "\n",
        "    rate.loc[(rate.anomalies > 0), 'anomalies']  = 1\n",
        "    rate.loc[(rate.anomalies.isna()), 'anomalies']  = 0\n",
        "\n",
        "    rate.to_csv(f'catch_{table}_sarima_predict.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def db2_sarima_anomalies_detect(dataframe, table):\n",
        "    print(f'Prepare ext_{table}_sarima_predict.csv')\n",
        "    \n",
        "    ids = pd.Series(dataframe[f'id_{table}']).unique()\n",
        "    rate = pd.DataFrame()\n",
        "    for id in ids:\n",
        "        aggr_df = dataframe.loc[dataframe[f'id_{table}']==id].groupby(\"date_fishery_x\")[[\"volume_x\"]].sum()\n",
        "        if aggr_df.size > 20:\n",
        "            anomalies = anomalies_bonds(aggr_df)    \n",
        "            aggr_df[f'id_{table}'] = id\n",
        "            aggr_df = pd.concat([aggr_df, anomalies], axis=1)\n",
        "            aggr_df = aggr_df.reset_index()\n",
        "            rate = pd.concat([rate, aggr_df], axis=0)\n",
        "\n",
        "    rate.loc[(rate.anomalies > 0), 'anomalies']  = 1\n",
        "    rate.loc[(rate.anomalies.isna()), 'anomalies']  = 0\n",
        "\n",
        "    rate.to_csv(f'ext_{table}_sarima_predict.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepare df_catch\n",
            "Prepare df_ext\n",
            "Prepare df_ext2\n",
            "Prepare catch_ves_sarima_predict.csv\n",
            "Prepare catch_own_sarima_predict.csv\n",
            "Prepare catch_fish_sarima_predict.csv\n",
            "Prepare ext_ves_sarima_predict.csv\n",
            "Prepare ext_own_sarima_predict.csv\n",
            "Prepare ext_fish_x_sarima_predict.csv\n",
            "Prepare ext_Plat_sarima_predict.csv\n"
          ]
        }
      ],
      "source": [
        "catch = '/home/savin/Documents/DEV/dataset_fish/Датасет/test_data/db1/catch.csv'\n",
        "ext1 = '/home/savin/Documents/DEV/dataset_fish/Датасет/test_data/db2/Ext.csv'\n",
        "ext2 = '/home/savin/Documents/DEV/dataset_fish/Датасет/test_data/db2/Ext2.csv'\n",
        "\n",
        "df_catch, df_ext = get_data(catch_path=catch, ext_path=ext1, ext2_path=ext2)\n",
        "\n",
        "catch_table = ('ves', 'own', 'fish')\n",
        "ext_table = ('ves', 'own', 'fish_x', 'Plat')\n",
        "\n",
        "for table in catch_table:\n",
        "    db1_sarima_anomalies_detect(df_catch, table)\n",
        "\n",
        "for table in ext_table:\n",
        "    db2_sarima_anomalies_detect(df_ext, table) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ood_detection.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "4b2fecc064183793f8c0ea93cbbf4db177fc7dac51d6ac04a6e4610799014929"
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
